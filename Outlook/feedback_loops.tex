\section{Open feedback loops}

Greedily minimizing the running costs at each instant of time might be suboptimal, as heavy costs possibly arising in the future, are neglected. If the value function is known, it turns out we can instead greedily minimize an alternative \say{local} cost function, which also takes into account long-term costs.

To be more explicit, consider the control problem over time span $ \closedInterval{0}{T} $ with initial state $ x_0 $. In view of the DPP \eqref{dpp}, an admissible control $ \alpha^* \in \controlspace $ with associated trajectory $ x^* $, is optimal, if and only if,
\begin{equation*}
	v(0, x_0) = \int\limits_{0}^{t} f(t, x^{*}(t), \alpha^{*}(t)) \  dt + v(t, x^{*}(t)) \ ,
\end{equation*}
holds for all $ t \in \closedInterval{0}{T} $. Put in other words, $ \alpha^* $ and $ x^* $ form an optimal control-trajectory pair, if and only if, the function
\begin{align*}
	g : \closedInterval{0}{T} &\to \mathbb{R} \\
	t &\mapsto \int\limits_{0}^{t} f(t, x^{*}(t), \alpha^{*}(t)) \  dt + v(t, x^{*}(t)) \ ,
\end{align*}
is constant. If the value function is also smooth, we can differentiate the function $ g $ over $ \openInterval{0}{T} $. Recall the definition of

\begin{equation*}
	H^a(t, x, p) \coloneqq p^T b(t, x, p) - f(t, x, p) \ ,
\end{equation*}
for an external input $ a \in \mathcal{A} $, a point in time $ t \in \closedInterval{0}{T} $, a state $ x \in \mathbb{R}^N $ and a gradient variable $ p \in \mathbb{R}^N $.
As $ g $ is constant, we get $ g^{\prime}(t) = 0 $ for a.e. $ t > 0 $, and explicitly computing the derivative of $ g $ yields
\begin{equation}
\label{control Hamiltonian}
	v_t(t, x^{*}(t)) = H^{\alpha^{*}(t)}(t, x^{*}(t), -D_x v(t, x^{*}(t))) \ .
\end{equation}

In view of theorem \ref{classical solution HJB}, the value function, on the other hand, satisfies \eqref{HJB}. Therefore

\begin{equation}
	\label{supremum Hamiltonian}
	v_t(t, x^{*}(t)) = H(t, x^{*}(t), -D_x v(t, x^{*}(t)) \ ,
\end{equation}

where $ H $ denotes the pointwise supremum of all $ H^{a} $, over all inputs $ a \in \mathcal{A} $. By combining the equations \eqref{control Hamiltonian} and \eqref{supremum Hamiltonian}, we get that
\begin{equation}
\label{control maximizer}
	H^{\alpha^{*}(t)}(t, x^{*}(t), -D_x v(t, x^{*}(t))) = H(t, x^{*}(t), -D_x v(t, x^{*}(t)) \ .
\end{equation}

Equation \eqref{control maximizer} reads $ \alpha^* $ and $ x^* $ form an optimal control-trajectory pair, if and only if, at almost every instant of time $ 0 < t < T $, the input $ \alpha^{*}(t) $ maximizes the function

\begin{equation*}
	a \mapsto H^{a}(t, x^{*}(t), -D_x v(t, x^{*}(t))) \ .
\end{equation*}

Owing to the latter necessary and sufficient optimality condition, we can construct such a control-trajectory pair as follows. Firstly, we determine an \emph{optimal feedback map}. That is, we determine a mapping $ S : \openInterval{0}{T} \times \mathbb{R}^N \to \mathcal{A} $, satisfying

\begin{equation*}
	S(t, x) \in \argmax_{a \in \mathcal{A}} H^{a}(t, x, -D_x v(t, x)) \ ,
\end{equation*}

for any $ t \in \openInterval{0}{T} $ and $ x \in \mathbb{R}^N $. Naturally, we must know the value function in advance. We elaborate in paragraph \ref{importance uniqueness}, how computing the value function involves uniqueness guarantees regarding the terminal value problem, as provided by our global comparison result. Then, we infer an optimal controlled trajectory $ x^{*} $ by solving the IVP

\begin{equation}
\label{feedback IVP}
	\begin{cases}
	x(0) &= x_0 \\
	\dot{x}(t) &= S(t, x(t)) \ .
	\end{cases}
\end{equation}

Finally, we obtain an optimal control by setting $ \alpha^{*} = S(\cdot, x^{*}) $. 

While finding an optimal control seems within easy reach, each of the steps, listed above, proves quite challenging to implement.

To begin with, determining the value function, i.e. solving the terminal value problem \eqref{HJB} is a formidable task. On one hand \say{\dots \ it seems impossible to solve such an equation directly} \cite{zhou}. On the other hand, \say{solving a partial differential equation [numerically] in \dots \ large dimension[s] is a very complicated task, easily beyond the possibilities of the last generation computers}, Falcone \cite{bardi2008optimal} reports. Latter is known as the \emph{curse of dimensionality}.

There is, in addition, the task of constructing a feedback map, which involves maximizing the function

\begin{equation*}
	a \mapsto H^{a}(t, x, -D_x v(t, x)) \ .
\end{equation*}

w.r.t. $ a \in \mathcal{A} $, for every fixed parameters $ t > 0 $ and $ x \in \mathbb{R}^N $. Depending on $ H^{a} $ and the considered input space, this might result in a highly non-trivial optimization problem. Bardi and Capuzzo-Dolcetta \cite{bardi2008optimal} also point out, one has to impose some regularity constraints upon the feedback map, to guarantee the IVP \eqref{feedback IVP} admits a solution.

Worryingly, all those problems already arise when a \emph{smooth} value function is considered. \say{If the value function is merely continuous,  \ldots \ the problem of finding sufficient conditions in this form is open to our knowledge} \cite{bardi2008optimal}.


